---
title: First Steps with Stan
subtitle: "Maud goes to Vegas"
author: Andrew B. Collier
location: eRum (Budapest)
date: 15 May 2018
title_logo_toptal: true
title_logo_exegetic: true
output: 
  revealjs::revealjs_presentation:
    self_contained: false
    transition: none
    reveal_plugins: ["notes"]
    template: datawookie-reveal-markdown-template.html
    css: datawookie-reveal-markdown.css
editor_options: 
  chunk_output_type: console
---

# {data-background="img/maud-introduction.svg" data-background-size="50%" data-background-position="left bottom"}

<!--
    Other resources:

    - http://rpubs.com/pviefers/CologneR [has interesting example of hierarchical model]
    - https://www.youtube.com/watch?v=s-9itaL1v-o "Extending the Power of R with Stan" [focus on rstanarm package]
    - https://vimeo.com/132156595
    - https://www.youtube.com/watch?v=qQFF4tPgeWI [Bob Carpenter. Background information. No demo of Stan.]
    - https://www.youtube.com/watch?v=uSjsJg8fcwY [Michael Betancourt - "Some Bayesian Modeling Techniques in Stan". Mostly linear models.]
    - https://www.youtube.com/watch?v=pHsuIaPbNbY [Michael Betancourt - "Efficient Bayesian inference with Hamiltonian Monte Carlo". Great background on MC.]

    - http://m-clark.github.io/docs/IntroBayes.html
    
    - https://matthewdharris.com/2016/10/18/estimating-a-beta-distribution-with-stan-hmc/
    - http://blackwell.math.yorku.ca/MATH6635/2016/Day17/MCMC_with_STAN_examples.html
    - https://stats.stackexchange.com/questions/165/how-would-you-explain-markov-chain-monte-carlo-mcmc-to-a-layperson
    - http://mlwhiz.com/blog/2015/08/19/MCMC_Algorithms_Beta_Distribution/
    - http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.13.7133&rep=rep1&type=pdf
    - https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/
    - https://link.springer.com/article/10.3758%2Fs13423-016-1015-8
-->

```{r setup, include=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA, fig.width = 10, fig.height = 6.5)

options(width = 120)

library(dplyr)
library(tidyr)
library(rstan)
library(ggplot2)
library(icon)
library(knitr)

# Plotting options for RStan.
#
rstan_gg_options(fill = "skyblue", color = "skyblue4", pt_color = "red")
```

<aside class="notes">
I'm Andrew Collier and I'm a Freelance Data Scientist at Toptal, which is a talent network for technical specialists. If anybody is interested in finding out more about Toptal, I'd be happy to chat afterwards.

Enough about me. I want you to meet somebody else.

Meet my friend Maud.

Maud is 71. She likes cats and gardening.

She also has a gambling problem.
</aside>

## {data-background="img/slot-machine.jpg"}

```{r include=FALSE}
paytable <- data.frame(
  payout = c(0, 1, 2, 5, 10, 20, 100),
  frequency = c(700, 166, 80, 33, 15, 5, 1),
  symbol = c("", rep(c("mouse", "cat"), 3)),
  repeats = c(0, 1, 1, 2, 2, 3, 3),
  stringsAsFactors = FALSE
) %>% mutate(
  symbols = purrr::map2(symbol, repeats, function(sym, n) {ifelse(sym == "", "", emo::ji(sym)) %>% rep(n) %>% paste(collapse = "")}),
  probability = frequency / sum(frequency),
  description = ifelse(repeats == 0, "", sprintf("%dx %s", repeats, symbol))
) %>% select(frequency, payout, symbols, description)
```

<aside class="notes">
Her weakness is slot machines. She's positively obsessed by them.

To be honest, Maud is generally rather obsessive. As you'll see, this will work in our favour.
</aside>

## {data-background="img/maud-love-r.svg" data-background-size="50%" data-background-position="left bottom"}

<aside class="notes">
Somewhat surprisingly, Maud is also a big fan of R.

It's been really, um interesting, to have her as a collaborator.
</aside>

## Maud's Burning Questions {.center}

- What is the hit rate?
- What is the RTP?
- What is the hit rate for each winning combination?

<aside class="notes">
Maud wants to know three things about her favourite game:

- what proportion of spins result in a win (the hit rate);
- what proportion of her wager gets paid back to her on average (the RTP); and
- how the hit rate is broken down across the various winning combinations.

If she could get her hands on the uncertainties in these estimates, that would be an added bonus.
</aside>

## Maud's Data (by Session)

```{r create_data, include=FALSE}
SESSIONS <- 100
AVGSPINS <- 20

# Probability of wagering 1, 2 or 3 coins per spin.
#
COINPROB = c(0.8, 0.15, 0.05)

# Seed chosen so that first session has fewer than 10 spins.
#
set.seed(538)

sessions <- tibble(
  spins = rpois(SESSIONS, AVGSPINS)
) %>% mutate(
  # Generate samples for individual spins.
  #
  details = purrr::pmap(., function(spins) {
    data.frame(
      spin = 1:spins,
      wager = sample(1:3, spins, replace = TRUE, prob = COINPROB)
    ) %>% mutate(
      payout = sample(paytable$payout, spins, TRUE, paytable$frequency) * wager
    )
  }),
  session = 1:SESSIONS
) %>% select(session, spins, details)

sessions <- inner_join(
  sessions,
  sessions %>%
    tidyr::unnest() %>%
    group_by(session) %>%
    summarise(
      hits = sum(payout > 0),
      wager = sum(wager),
      payout = sum(payout)
    )
) %>%
  mutate(
    hit_rate = hits / spins,
    rtp = payout / wager
  ) %>%
  select(session, details, everything())
```

```{r}
sessions %>% select(-details)
```

<aside class="notes">
Fortunately for us Maud obsessively records her gambling stats. As a result we have a rather nice, rich data set to play with. At the session level the most important bits of information are the number of spins and the associated number of hits. It might occur to you that the relationship between these two quantities could be modelled as a binomial process.
</aside>

## Maud's Data (by Spin)

```{r}
sessions %>% select(session, details) %>% unnest()
```

<aside class="notes">
We can also unpack these data down to the level of individual spins, where we have something that could be modelled as a Bernoulli process.
</aside>

# Binomial Process {.center}

<aside class="notes">
We're going to focus on the session level data. It seems that we will be able to treat this as a Binomial Process, so let's take a moment to review.
</aside>

## {.center}

Probability distribution for binomial process:

$$
P(k | n, \theta) = \binom{n}{k} \theta^k (1 - \theta)^{n - k}
$$

where

- $k$ successes in $n$ trials where
- the probability of success on any trial is $\theta$.

<aside class="notes">
For each session we are considering the number of spins (or "trials") denoted by $n$ and the number of hits (or "successes") denoted by $k$. The probability of success on any spin is $\theta$.

The binomial distribution allows us to calculate the probability of $k$ given specific values for $n$ and $\theta$. So this assumes that you know $\theta$... But what if you don't? Certainly for most slot machines you don't know the probability of winning. If you did then you probably wouldn't be very tempted to play.

Suppose we had observations of $k$ and $n$, and wanted to estimate $\theta$.
</aside>

## {.center}

$$
\text{hit rate} = \theta^* = \frac{\text{hits}}{\text{spins}}
$$

<aside class="notes">
Well, the hit rate, $\theta$, is just the expected number of hits per spin.
</aside>

## {.center}

```{r}
with(sessions, sum(hits) / sum(spins))
```

<aside class="notes">
So we could lump all of the data together and treat it as a single massive binomial experiment, taking the ratio of the total number of hits to the total number of spins across all sessions.

This would certainly maximise the amount of data and should, in principle, give us a reasonably accurate result. But there is no way to quantify our uncertainty. Such a paucity of information leads to overconfidence in the quality of the estimate.
</aside>

## {.center}

```{r hit_rate_histogram, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3}
ggplot(sessions, aes(x = hit_rate)) +
  geom_histogram(binwidth = 0.05) +
  scale_x_continuous("Hit Rate per Session", limits = c(0, 1), labels = scales::percent) +
  scale_y_continuous("Frequency") +
  theme_classic()

session_summary <- sessions %>% summarise(
  session_avg = mean(hit_rate),
  session_std = sd(hit_rate)
) %>% mutate(
  ci_lower = session_avg - 2 * session_std / sqrt(SESSIONS),
  ci_upper = session_avg + 2 * session_std / sqrt(SESSIONS)
)
```

```{r echo=FALSE}
session_summary %>% select(session_avg, session_std)
```

95% confidence interval () extends from `r format(session_summary$ci_lower * 100, digits = 3)`% to  `r format(session_summary$ci_upper * 100, digits = 3)`%.

<aside class="notes">
Another approach is to treat each session as an independent binomial experiment, each of which yields an estimate for the hit rate.

Now you can calculate the average and spread. However, there are two problems with this:

1. it ignores the number of spins per session (this could be incorporated by using weighted statistics) and
2. it assumes that the hit rate is Normally distributed (and it's not!).

Maud is fairly happy with this result, but it does concern her that the confidence interval on the hit rate could, in principle, extend to values less than zero or greater than one. Neither of which is physically possible. There are no constraints on $\theta$.
</aside>

---

```{r include=FALSE}
y <- c(0, 1, 0, 0, 1, 1, 0)

# Log-likelihood of a parameter value given the data.
#
bernoulli_log_likelihood <- function(theta, y) {
  sum(y * log(theta) + (1 - y) * log(1 - theta))
}

bernoulli_log_likelihood(0.3, y)
bernoulli_log_likelihood(0.6, y)
```

```{r include=FALSE}
binom_probability <- function(theta, k, n) {
  prod(choose(n, k) * theta^k * (1 - theta)^(n - k))
  #
  # Equivalent to dbinom(k, size = n, prob = theta)
}
```

For session $i$ there were $k_i$ hits from $n_i$ spins.

Assuming that sessions are independent:

$$
P(k | n, \theta) = \prod_i P(k_i | n_i, \theta) = L(\theta; n, k)
$$

<aside class="notes">
Let's consider what the probability distribution looks like for all of the sessions lumped together. If we assume that they are independent, which is a reasonable assumption, then we can simply multiply together the probability distributions for each session. This gives us, one the one hand, a distribution for the number of hits in each of the sessions. But if we think of this as a function of $\theta$ then this gives the likelihood of a particular value of $\theta$ given observed values of $k_i$ and $n_i$.

If we maximise this function then we have the most likely value of $\theta$ based on all of the data.

**Note:**

$P(k | n, \theta)$ and $L(\theta; n, k)$ are functionally equivalent but

- $P(k | n, \theta)$ is a function of $k$ (and is a probability distribution) and
- $L(\theta; n, k)$ is a function of $\theta$ (and is *not* a probability distribution).
</aside>

---

Log-likelihood for binomial process (multiple trials):

$$
LL(\theta; n, k) = \sum_i k_i \log{\theta} + (n_i - k_i) \log{(1 - \theta)}
$$

```{r echo=FALSE}
binom_likelihood <- function(theta, k, n) {
  prod(theta^k * (1 - theta)^(n - k))
}
```

```{r}
# theta - probability [single value]
# k - number of successes [vector]
# n - number of trials [vector]
#
binom_log_likelihood <- function(theta, k, n) {
  sum(k * log(theta) + (n - k) * log(1 - theta))
}
```

<aside class="notes">
Neglect the binomial coefficient since it's a constant (as a function of theta).

The log likelihood is generally easier to work with because it gets around the problem of multiplying together many small numbers.

Because the logarithm is a strictly increasing function, the log likelihood is maximised in the same place as the likelihood.
</aside>

---

```{r include=FALSE}
binom_log_likelihood(0.6, sessions$hits, sessions$spins)
binom_log_likelihood(0.6, sessions$hits, sessions$spins)

parameters <- tibble(theta = seq(0, 1, 0.02)) %>%
  mutate(
    log_likelihood = sapply(theta, binom_log_likelihood, sessions$hits, sessions$spins)
  )
```

```{r binomial_log_likelihood, echo=FALSE, fig.height=3.5}
ggplot(parameters, aes(theta, log_likelihood)) +
  geom_point() +
  scale_y_continuous("Log likelihood") +
  theme_classic()
```

```{r}
parameters %>% arrange(desc(log_likelihood)) %>% head()
```

<aside class="notes">
In the Maximum Likelihood approach we simply select the value of the parameter which gives the largest likelihood.

The assumption implicit in this approach is that in the absence of data all parameter values are equally probable.

This gives us a point estimate for the parameter. There is no indication of uncertainty. However, the likelihood is a tool which will allow us to get a much better characterisation of the hit rate.
</aside>

# Bayes {.center}

## Bayesian Inference

Bayes' Theorem
$$
p(\theta|y, X) = \frac{p(y|X, \theta) p(\theta)}{p(y)} \propto p(y|\theta) p(\theta)
$$
where

- $y$ are observations;
- $X$ are predictors;
- prior — $p(\theta)$ is parameter distribution <em>before</em> data;
- likelihood — $p(y|X, \theta)$ is probability of data <em>given</em> parameters; and
- posterior — $p(\theta|y, X)$ is parameter distribution <em>given</em> data.

<aside class="notes">
A Bayesian model is the combination of a prior and a likelihood.

The likelihood characterises the model and gives the probability of the data conditional on the model and choice of parameter.

The prior is specifies what we know about the model parameter before considering the data.

The denominator is sometimes called the "Evidence". It's just the marginal distribution of $y$.

It's also taken a while for these techniques to take off because they are computationally challenging. Hardware advances have made them eminently feasible.

So much for theory. Analytical expressions are rare in practice.

Confounding features:

- data are often multi-dimensional;
- models have multiple parameters.

So evaluating $p(\theta|y, X)$ becomes challenging!
</aside>

## {.center}

<blockquote>
... the theory of inverse probability is founded upon error and must be wholly rejected.
<cite>Sir Ronald Fisher (1925)</cite>
</blockquote>

<aside class="notes">
Historically the Bayesian approach has been marginalised by some eminent statisticians. For example, Ronald Fisher had this to say about inverse probability as Bayesian inference was known in his day. Evidently not everybody is a fan.

In general you can't apply Bayesian techniques analytically.
</aside>

---

```{r include=FALSE}
prior <- tibble(
  theta = seq(0, 1, 0.02),
  update = 0,
  prior = NA
)

prior_unif <- prior %>% mutate(
  posterior = 1,
  # Normalise.
  posterior = posterior / sum(posterior)
)
prior_beta <- prior %>% mutate(
  posterior = dbeta(theta, 8, 4),
  # Normalise.
  posterior = posterior / sum(posterior)
)

rm(prior)

binom_update <- function(prior, k, n) {
  likelihood <- sapply(prior$theta, binom_likelihood, k, n)
  #
  prior %>% mutate(
    prior = posterior,
    # Update.
    posterior = likelihood * prior,
    # Normalise.
    posterior = posterior / sum(posterior)
  )
}

# plot(p ~ theta, prior_unif)
# plot(p ~ theta, binom_update(prior_unif, 1, 3))

# plot(p ~ theta, prior_beta)
# plot(p ~ theta, binom_update(prior_beta, 1, 3))
```

```{r binomial_updates_uniform, echo=FALSE, message=FALSE, warning=FALSE}
for (n in 1:11) {
  hits = sessions[[n, "hits"]]
  spins = sessions[[n, "spins"]]
  prior_unif <- rbind(prior_unif, binom_update(prior_unif %>% filter(update == n - 1), hits, spins) %>% mutate(update = update + 1))
  prior_beta <- rbind(prior_beta, binom_update(prior_beta %>% filter(update == n - 1), hits, spins) %>% mutate(update = update + 1))
}

ggplot(prior_unif, aes(x = theta)) +
  geom_line(aes(y = prior), lty = "dashed") +
  geom_line(aes(y = posterior)) +
  geom_point(aes(y = posterior), size = 1) +
  facet_wrap(~ update, scales = "free_y") +
  scale_y_continuous("") +
  scale_x_continuous(breaks = seq(0, 1, 0.2)) +
  theme_classic() +
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())
```

<aside class="notes">
One approach to applying Bayes' Theorem is to evaluate the posterior across a regular grid. Not surprisingly this is known as the "Grid Approximation".

In the grid approximation we consider a finite number of points at which we evaluate the prior and likelihood. Their product then gives us the posterior at each of those points. We can then use Bayes' Theorem to successively update the prior with new data.

At each iteration the model effectively learns from the new data. The distribution shifts and becomes narrower. Also note that each iteration uses the results of the previous iteration as a prior.

There's a major problem with the grid approximation: it scales poorly with the number of parameters in the model and rapidly becomes computationally intractable.
</aside>

```{r binomial_updates_beta, eval=FALSE, include=FALSE}
ggplot(prior_beta, aes(x = theta, y = p)) +
  geom_line() +
  geom_point(size = 1) +
  facet_wrap(~ update, scales = "free_y") +
  theme_classic()
```

# Monte Carlo {.center}

<aside class="notes">
Monte Carlo provides a more efficient technique for exploring a parameter space.

The approach draw samples from the posterior at random.
</aside>

## Vanilla Monte Carlo

Generate independent samples of $\theta^{(i)}$.

To do this we need to have $p(\theta^{(m)} | y, X)$.

```{r echo=FALSE, warning=FALSE}
set.seed(13)

parameters <- tibble(
  x1 = c(NA, rnorm(20)),
  y1 = c(NA, rnorm(20))
) %>% mutate(
  x2 = c(na.omit(x1), NA),
  y2 = c(na.omit(y1), NA)
)

ggplot(parameters) +
  geom_point(aes(x = x1, y = y1), color = "red", size = 3) +
  labs(x = "", y = "") +
  theme_classic() +
  theme(axis.text = element_blank(), axis.ticks = element_blank(), axis.line = element_blank(), aspect.ratio = 1)
```

<aside class="notes">
Conventional Monte Carlo generates independent, random samples from a distribution.

The objective is to jump around parameter space in such a way that the probability of being at a point is proportional to a distribution.

Efficient algorithms exist for doing this with simple distributions. However, any non-trivial distribution will generally require you to apply something like rejection sampling. And to make that efficient we'd need to know the maximum value of the distribution.

However there's an alternative...
</aside>

## Markov Chain Monte Carlo (MCMC)

Generate a series of samples: $\theta^1$, $\theta^2$, $\theta^3$, ... $\theta^M$ where $\theta^m$ depends on $\theta^{m-1}$.

To do this we need to have $p(\theta^{m} | \theta^{m-1}, y, X)$.

```{r echo=FALSE, warning=FALSE}
ggplot(parameters) +
  geom_segment(aes(x = x1, y = y1, xend  = x2, yend = y2), lty = "dashed") +
  geom_point(aes(x = x1, y = y1), color = "red", size = 3) +
  labs(x = "", y = "") +
  theme_classic() +
  theme(axis.text = element_blank(), axis.ticks = element_blank(), axis.line = element_blank(), aspect.ratio = 1)
```

<aside class="notes">
Markov Chain Monte Carlo generates a sequence of samples from a distribution. The sames are no longer independent, but have the Markovian property that any sample only depends on the previous sample.
</aside>

## Metropolis-Hastings Algorithm

Randomly sample $\theta^{(0)}$.

Set $i = 1$.

1. Randomly sample proposal $\theta^{*}$ in the vicinity of $\theta^{i-1}$.
2. Sample $u$ uniformly on $[0, 1)$.
3. 

$$
\theta^{(i)} = \left\{\begin{array}{ll}
\theta^{*} & \text{if } u \cdot p(\theta^{i-1}|y, X) < p(\theta^{*}|y, X) \\
\theta^{(i-1)} & \text{otherwise.}
\end{array}\right.
$$

4. Return to 1.

<aside class="notes">
The Metropolis-Hastings Algorithm is the classical implementation of MCMC.  The algorithm proceeds by sampling a proposal point in the vicinity of the current point and then either accepting or rejecting that point depending on the ratio of the probability density at the two points. If the probability of the new point is higher then we always accept. If it's lower then we accept with a probability that is the ratio of the densities.

It might occur to you that the samples are now no longer independent. And you'd be completely correct. We'll see how this is accounted for.

Metropolis Monte Carlo is better than the Grid Approximation. However in an absolute sense it's not really all that good. Especially not in higher dimensions, where it can take a very long time to sample the parameter space.

**Notes:**

The size of the "vicinity" (effectively the step size) determines the acceptance rate of new points. If it's too large then few new points will be accepted. If it's too small then the rate of diffusion will be painfully slow.

Without step 3 this would essentially give us diffusion, a random walk. But this step, the Metropolis Acceptance Procedure, ensures that sampling converges to the underlying distribution.

The principle advantage of MCMC is that we don't need to know the maximum value of the posterior.

Also it only depends on the ratio of posteriors, so these do not need to be normalised.
</aside>

```{r eval=FALSE, include=FALSE}
# A simple implementation of the Metropolis-Hastings algorithm.

N = 4000

proposal <- function(theta) {
  rnorm(1, theta)
}

posterior <- function(theta) {
  dbeta(theta, 7, 4)
}

chain <- vector(mode = "numeric", length = N)
chain[1] = 0.5
#
for (n in 2:N) {
  star = proposal(chain[n-1])
  u = runif(1)
  #
  if (u < posterior(star) / posterior(chain[n-1])) {
    chain[n] = star
  } else {
    chain[n] = chain[n-1]
  }
}

chain = data.frame(
  n = 1:N,
  theta = chain
) %>% mutate(
  batch = (n - 1) %/% 1000 
)

ggplot(chain, aes(x = n, y = theta)) +
  geom_line() +
  scale_y_continuous(limits = c(0, 1))
```

## {.center}

<img src="https://rawgit.com/DataWookie/datawookie/master/static/img/logo/logo-stan.svg" />

![](https://rawgit.com/DataWookie/datawookie/master/static/img/logo/logo-stan.svg)

<aside class="notes">
Stan is a high level language for writing statistical models. It uses Hamiltonian Monte Carlo, which applies some general principles from Physics to provide much more efficient sampling. Yes, it takes a little longer to generate each sample, but those samples are guaranteed to explore parameter space much more effectively.

**Notes:**

The Hamiltonian MC is *much* more efficient than vanilla MCMC or the Metropolis Algorithm.
</aside>

---

Stan workflow:

1. Data story (informs the model)
2. Write Stan program (likelihood and priors).
3. Stan parser converts this to C++.
4. Compile C++.
5. Execute compiled binary.
6. Evaluate results. (Possibly return to 2.)
7. Inference based on posterior sample.

Using Stan with R:

- A `.R` file. USE FONTAWESOME FILE SYMBOL!!!
- A `.stan` file. USE FONTAWESOME FILE SYMBOL!!!

<aside class="notes">
1. A data story will motivate the Stan model.
2. Encode the model in Stan modeling language.
3. Translate the Stan program to C++ code.
4. Compile the C++ code.
5. Use the compiled code to sample from the posterior distribution. This is where the magic happens.
6. Ensure that results are meaningful. In not, iterate.
7. Use the posterior sample to do inference.

Stan is a standalone system. However it's a lot easier to run it from within R.
</aside>

## Stan Skeleton

<script src="https://gist.github.com/DataWookie/61d7a662ecaad87044e1c9f161a5245f.js"></script>

<aside class="notes">
A Stan file is comprised of a number of blocks, not all of which are required.

The most important blocks are:

- `data` which specifies the characteristics of the data;
- `parameters` which lists the parameters of the model; and
- `model` which defines the likelihood and priors.
</aside>

## {.center}

```{r echo=FALSE, cache=FALSE}
read_chunk("binomial-uniform-prior.stan")
```

```{text stan_binomial_uniform_prior, message=FALSE, warning=FALSE, eval=FALSE}
```

<aside class="notes">
We'll start by building a model to address Maud's first burning question: what is the hit rate.

The data that we are providing to Stan consists of two vectors, `hits` and `spins`.

The model specifies that we are assuming a binomial relationship between `hits` and `spins` with a success probability denoted by the parameter `theta`. The ~ represents a stochastic relationship. An = would indicate a deterministic relationship.

We know that `theta` must lie between 0 and 1. In the absence of any further information we specify a uniform prior: any value of `theta` is equally likely. If we did not specify a prior then Stan would actually use the uniform prior by default.
</aside>

## {.center}

```{r message=FALSE, warning=FALSE, results='hide', cache=TRUE}
library(rstan)

# Use all available cores.
#
options(mc.cores = parallel::detectCores())

trials <- list(
  N       = nrow(sessions),
  hits    = sessions$hits,
  spins   = sessions$spins
)

fit <- stan(
  file    = "binomial-no-prior.stan",
  data    = trials,
  chains  = 2,                         # Number of Markov chains
  warmup  = 500,                       # Warmup iterations per chain
  iter    = 1000                       # Total iterations per chain
)
```

<aside class="notes">
This is the corresponding R code. Things to note:

- it loads the `rstan` library;
- sets the number of cores to be used for parallel execution (otherwise chains are run in series);
- data are transferred to Stan via a list (with corresponding names);
- Stan is invoked via the `stan()` function; and
- parameters specify the number of chains as well as number of warmup and compute iterations per chain.

The "chains" are initially a little mysterious, but these are basically just independent series of samples. They aid better sampling of parameter space and also facilitate parallelism.

Note that there are 1000 iterations in total per chain, of which 500 are warmup, which leaves 500 active iterations per chain. So, in total there are 1000 (2 times 500) active iterations.
</aside>

---

```{r traceplot_binomial, echo=FALSE}
traceplot(fit, pars = c("theta"), inc_warmup = TRUE)
```

<aside class="notes">
A traceplot shows the values sampled for a particular parameter as a function of iteration number.

The chains are uncorrelated, but successive samples within each chain are clearly related to each other. Ideally we are aiming for good "mixing" across the domain of the distribution.

What's happening during the warmup phase? The parameters of the MC are getting tuned to optimally explore the parameter space (one aspect of this is choosing the step size).
</aside>

---

```{r eval=FALSE, include=FALSE}
samples <- extract(fit)

names(samples)

samples$theta %>% head()
```

```{r}
class(fit)

samples <- as.data.frame(fit)

head(samples)

nrow(samples)
```

<aside class="notes">
We don't actually get the posterior directly. The `stanfit` object is really just a collection of samples from the posterior. We can extract those samples using the `extract()` function, which yields a list. It's normally easier to just convert these samples to a data frame.

The number of samples that we get is precisely the total number of active iterations across all chains.
</aside>

## {.center}

```{r density_binomial, eval=FALSE, include=FALSE}
plot(fit, show_density = TRUE, ci_level = 0.5, fill_color = "purple")
```

```{r hist_binomial, echo=FALSE}
plot(fit, plotfun = "hist", pars = c("theta"), include = TRUE, binwidth = 0.0025)
```

## {.center}

```{r autocorrelation, echo=FALSE}
# Autocorrelation between successive samples.
#
plot(fit, pars = "theta", plotfun = "ac")
```

<aside class="notes">
The nature of MCMC is that successive samples of the parameters are correlated and we can see that clearly in an autocorrelation plot. We'll see in a moment that this has an effect on the interpretation of the results.
</aside>

## {.center}

```{r}
print(fit, probs = c(0.025, 0.5, 0.975))
```

Use `summary()` to get information for each chain.

<aside class="notes">
Let's look at the summary information.

There's the mean value of `theta` as well as the lower and upper quantiles of the distribution. Don't think of a confidence interval. These are quantiles of the density. Close to the median they are stable, but further into the tails they can be noisy.

The average hit rate is in good agreement with our earlier result.

`n_eff` is the effective sample size. This takes into account that the samples are not independent (this is a consequence of correlation in the Markov Chain). This is a vital component of the Monte Carlo version of the Central Limit Theorem.

Maud is immediately more comfortable with this result because, even with a uniform prior, the success rate is constrained to a reasonable domain between 0 and 1.
</aside>

## {.center}

```{r echo=FALSE, cache=FALSE}
read_chunk("binomial-beta-prior.stan")
```

```{text stan_binomial_beta_prior, message=FALSE, warning=FALSE, eval=FALSE}
```

<aside class="notes">
As an experienced gambler Maud knew that the likelihood of the hit rate was not uniformly distributed between 0 and 1. We could factor this knowledge into the computation by applying a more informed prior. We could use a broad beta distribution peaked at 0.5.

**Notes:**

The beta distribution is the conjugate prior for the binomial likelihood (this means that the corresponding posterior will also be a beta distribution).
</aside>

---

```{r message=FALSE, warning=FALSE, cache=TRUE, include=FALSE, results='hide'}
library(rstan)

trials <- list(
  N       = nrow(sessions),
  hits    = sessions$hits,
  spins   = sessions$spins
)

fit <- stan(
  file    = "binomial-beta-prior.stan",
  data    = trials,
  chains  = 4,
  warmup  = 1000,
  iter    = 2000
)
```

<aside class="notes">
</aside>

## {.center}

```{r hist_rtp, echo=FALSE, cache=TRUE}
ggplot(sessions, aes(x = rtp)) +
  geom_histogram(binwidth = 0.2, boundary = 0, fill = "skyblue") +
  geom_vline(xintercept = c(0.5, 1.0), lty = "dashed") +
  scale_x_continuous("RTP", labels = scales::percent) +
  scale_y_continuous("Count") +
  theme_classic()
```

<aside class="notes">
Let's move onto Maud's second burning question, quantifying the RTP. We can see that there is appreciable variation in RTP between sessions. We'd like to quantify the mean RTP and get an idea of the associated uncertainties.

Now this distribution is definitely not Normal. The long tail is a dead giveaway. But we are going to treat it as Normal for the moment. Just humour me.
</aside>

## {.center}

```{r echo=FALSE, cache=FALSE}
read_chunk("normal.stan")
```

```{text stan_normal, message=FALSE, warning=FALSE, eval=FALSE}
```

<aside class="notes">
Now our Stan code specifies a Normal for the likelihood, with two parameters, `mu` and `sigma`, being the mean and standard deviation.

We're not going to specify a prior for `sigma` but simply constrain it to being a positive real number.

We will give a prior for `mu` though, using a broad beta distribution with a peak at a half. This seems not unreasonable. We know that the RTP is somewhere between 0 and 1.
</aside>

```{r cache=TRUE, include=FALSE}
trials <- list(
  N       = nrow(sessions),
  rtp     = sessions$rtp
)

fit <- stan(
  file    = "normal.stan",
  data    = trials,
  chains  = 4,
  warmup  = 1000,
  iter    = 2000
)
```

## {.center}

```{r pairs_rtp, echo=FALSE}
pairs(fit, pars = c("mu", "sigma", "lp__"), las = 1)
```

<aside class="notes">
A pairs plot lets us interrogate the results. It's clear that both of the model parameters are correlated with the log likelihood, but apparently independent of each other. We've assumed that the parameters are independent, so this is good.
</aside>

## {.center}

```{r echo=FALSE}
fit
```

```{r include=FALSE}
rtp_summary = data.frame(summary(fit)$summary)
```

The average RTP is around `r sprintf("%.0f", rtp_summary$mean[1] * 100)`%.

<aside class="notes">
We get reasonable values for the two parameters. However, Maud is concerned about using a Normal distribution for modelling RTP. The left tail of the distribution extends to negative values, which is not physically realisable for RTP.

We can do better!
</aside>

## {.center}

```{r echo=FALSE}
paytable %>% select(description, symbols, payout) %>% kable("html")
```

---

```{r echo=FALSE, cache=FALSE}
read_chunk("normal-rtp.stan")
```

```{text stan_normal_rtp, message=FALSE, warning=FALSE, eval=FALSE}
```

<aside class="notes">
The priors are based on Maud's gut feel for the relative frequency of each of the payouts. They carry a little information about the probabilities of each payline, but not a lot.
</aside>

---

```{r cache=TRUE, include=FALSE}
trials <- list(
  N       = nrow(sessions),
  rtp     = sessions$rtp
)

fit <- stan(
  file    = "normal-rtp.stan",
  data    = trials,
  chains  = 4,
  warmup  = 1000,
  iter    = 2000
)

rtp_summary = data.frame(summary(fit)$summary)
```

```{r traceplot_rtp, echo=FALSE}
traceplot(fit, pars = paste0("mu", 1:6), inc_warmup = TRUE, nrow = 3)
```

<aside class="notes">
It's readily apparent here the importance of the warmup iterations: the first few samples for some of the parameters are very far from the core of their distribution. This could be improved by providing more restrictive priors.
</aside>

---

```{r}
summary(fit)$summary
```

The overall RTP is `r format(rtp_summary["mu", "mean"] * 100, digits = 3)`.

<aside class="notes">
</aside>

## Predictive Inference

## Posterior Predictive Distribution

$$
p(\tilde{y}|y) = \int_\theta p(\tilde{y}|\theta) p(\theta|y) d\theta
$$

## What is Bayes Good For?

- Models where we care about parameter values.
- Models where we want to quantify uncertainty in parameter values.
- Generate some derived quantity from the posterior (because we can evaluate any function of the parameters...)

<aside class="notes">
So these are not black box Machine Learning models where we simply want to make the best possible predictions. Here we actually care what's going on inside the box.
</aside>

## Resources

- http://mc-stan.org/workshops/
- <a href="http://mc-stan.org/">Stan web site</a>
- <a href="http://www.mcmchandbook.net/HandbookChapter5.pdf">MCMC Using Hamiltonian Dynamics</a> (Radford Neal)
- Blogs:
  - <http://andrewgelman.com/>

<!-- ============================================================================================================== -->
<!-- ===                                                                                                        === -->
<!-- === END OF TALK                                                                                            === -->
<!-- ===                                                                                                        === -->
<!-- ============================================================================================================== -->

# Extra Stuff

## Poisson

```{r stan_poisson, message=FALSE, warning=FALSE, results='hide', cache=TRUE}
# trials <- list(
#   N       = nrow(sessions),
#   spins   = sessions$spins
# )
# 
# fit <- stan(
#   file    = "poisson.stan",
#   data    = trials,
#   chains  = 4,
#   warmup  = 1000,
#   iter    = 2000
# )
```

---

```{r}
# extract(fit)

# hist(extract(fit)$lambda)
```

<aside class="notes">
</aside>

## Regression

```{r}
# ggplot(sessions, aes(spins, wager)) + geom_jitter()
```

## Regression Stan

```{r echo=FALSE, cache=FALSE}
read_chunk("regression.stan")
```

```{text stan_regression, message=FALSE, warning=FALSE, eval=FALSE}
```

<aside class="notes">
Because the number of spins has been standardised, the intercept will be the wager for the average number of spins.
</aside>

## Regression Stan R

```{r cache=TRUE, include=FALSE}
# trials <- list(
#   N       = nrow(sessions),
#   x       = scale(sessions$spins)[,1], # Standardised covariate
#   y       = sessions$wager
# )
# 
# fit <- stan(
#   file    = "regression.stan",
#   data    = trials,
#   chains  = 4,
#   warmup  = 1000,
#   iter    = 2000
# )
```

```{r}
# summary(fit)
```

<aside class="notes">
In a linear model we are saying that the response is normally distributed with a mean that depends on the covariates and a constant standard deviation. Assuming homogeneous noise.

The half-Cauchy is essentially a one-sided, heavy tailed version of the Normal distribution. This constrains the value of sigma to be positive but is not as restrictive as a Normal distribution.
</aside>

## ShinyStan

library(shinystan)
launch_shinystan(fit)
#
# Diagnose -> PPcheck "Posterior predictive check" (look at distribution of observed data versus replications - do they look similar? "If our model is a good fit then we should be able to use it to generate data that looks a lot like the data we observed.")