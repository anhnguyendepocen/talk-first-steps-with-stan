<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Andrew B. Collier" />
  <meta name="dcterms.date" content="2018-05-15" />
  <title>First Steps with Stan</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="bayesian-leap_files/reveal.js-3.3.0.1/css/reveal.css"/>
  <link href="https://use.fontawesome.com/releases/v5.0.7/css/all.css" rel="stylesheet">


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="bayesian-leap_files/reveal.js-3.3.0.1/css/theme/simple.css" id="theme">


  <!-- some tweaks to reveal css -->
  <style type="text/css">
    .reveal h1 { font-size: 2.0em; }
    .reveal h2 { font-size: 1.5em;  }
    .reveal h3 { font-size: 1.25em;	}
    .reveal h4 { font-size: 1em;	}

    .reveal .slides>section,
    .reveal .slides>section>section {
      padding: 0px 0px;
    }



    .reveal table {
      border-width: 1px;
      border-spacing: 2px;
      border-style: dotted;
      border-color: gray;
      border-collapse: collapse;
      font-size: 0.7em;
    }

    .reveal table th {
      border-width: 1px;
      padding-left: 10px;
      padding-right: 25px;
      font-weight: bold;
      border-style: dotted;
      border-color: gray;
    }

    .reveal table td {
      border-width: 1px;
      padding-left: 10px;
      padding-right: 25px;
      border-style: dotted;
      border-color: gray;
    }


  </style>

    <style type="text/css">code{white-space: pre;}</style>

    <link rel="stylesheet" href="datawookie-reveal-markdown.css"/>

<!-- Printing and PDF exports -->
<script id="paper-css" type="application/dynamic-css">

/* Default Print Stylesheet Template
   by Rob Glazebrook of CSSnewbie.com
   Last Updated: June 4, 2008

   Feel free (nay, compelled) to edit, append, and
   manipulate this file as you see fit. */


@media print {

	/* SECTION 1: Set default width, margin, float, and
	   background. This prevents elements from extending
	   beyond the edge of the printed page, and prevents
	   unnecessary background images from printing */
	html {
		background: #fff;
		width: auto;
		height: auto;
		overflow: visible;
	}
	body {
		background: #fff;
		font-size: 20pt;
		width: auto;
		height: auto;
		border: 0;
		margin: 0 5%;
		padding: 0;
		overflow: visible;
		float: none !important;
	}

	/* SECTION 2: Remove any elements not needed in print.
	   This would include navigation, ads, sidebars, etc. */
	.nestedarrow,
	.controls,
	.fork-reveal,
	.share-reveal,
	.state-background,
	.reveal .progress,
	.reveal .backgrounds {
		display: none !important;
	}

	/* SECTION 3: Set body font face, size, and color.
	   Consider using a serif font for readability. */
	body, p, td, li, div {
		font-size: 20pt!important;
		font-family: Georgia, "Times New Roman", Times, serif !important;
		color: #000;
	}

	/* SECTION 4: Set heading font face, sizes, and color.
	   Differentiate your headings from your body text.
	   Perhaps use a large sans-serif for distinction. */
	h1,h2,h3,h4,h5,h6 {
		color: #000!important;
		height: auto;
		line-height: normal;
		font-family: Georgia, "Times New Roman", Times, serif !important;
		text-shadow: 0 0 0 #000 !important;
		text-align: left;
		letter-spacing: normal;
	}
	/* Need to reduce the size of the fonts for printing */
	h1 { font-size: 28pt !important;  }
	h2 { font-size: 24pt !important; }
	h3 { font-size: 22pt !important; }
	h4 { font-size: 22pt !important; font-variant: small-caps; }
	h5 { font-size: 21pt !important; }
	h6 { font-size: 20pt !important; font-style: italic; }

	/* SECTION 5: Make hyperlinks more usable.
	   Ensure links are underlined, and consider appending
	   the URL to the end of the link for usability. */
	a:link,
	a:visited {
		color: #000 !important;
		font-weight: bold;
		text-decoration: underline;
	}
	/*
	.reveal a:link:after,
	.reveal a:visited:after {
		content: " (" attr(href) ") ";
		color: #222 !important;
		font-size: 90%;
	}
	*/


	/* SECTION 6: more reveal.js specific additions by @skypanther */
	ul, ol, div, p {
		visibility: visible;
		position: static;
		width: auto;
		height: auto;
		display: block;
		overflow: visible;
		margin: 0;
		text-align: left !important;
	}
	.reveal pre,
	.reveal table {
		margin-left: 0;
		margin-right: 0;
	}
	.reveal pre code {
		padding: 20px;
		border: 1px solid #ddd;
	}
	.reveal blockquote {
		margin: 20px 0;
	}
	.reveal .slides {
		position: static !important;
		width: auto !important;
		height: auto !important;

		left: 0 !important;
		top: 0 !important;
		margin-left: 0 !important;
		margin-top: 0 !important;
		padding: 0 !important;
		zoom: 1 !important;

		overflow: visible !important;
		display: block !important;

		text-align: left !important;
		-webkit-perspective: none;
		   -moz-perspective: none;
		    -ms-perspective: none;
		        perspective: none;

		-webkit-perspective-origin: 50% 50%;
		   -moz-perspective-origin: 50% 50%;
		    -ms-perspective-origin: 50% 50%;
		        perspective-origin: 50% 50%;
	}
	.reveal .slides section {
		visibility: visible !important;
		position: static !important;
		width: auto !important;
		height: auto !important;
		display: block !important;
		overflow: visible !important;

		left: 0 !important;
		top: 0 !important;
		margin-left: 0 !important;
		margin-top: 0 !important;
		padding: 60px 20px !important;
		z-index: auto !important;

		opacity: 1 !important;

		page-break-after: always !important;

		-webkit-transform-style: flat !important;
		   -moz-transform-style: flat !important;
		    -ms-transform-style: flat !important;
		        transform-style: flat !important;

		-webkit-transform: none !important;
		   -moz-transform: none !important;
		    -ms-transform: none !important;
		        transform: none !important;

		-webkit-transition: none !important;
		   -moz-transition: none !important;
		    -ms-transition: none !important;
		        transition: none !important;
	}
	.reveal .slides section.stack {
		padding: 0 !important;
	}
	.reveal section:last-of-type {
		page-break-after: avoid !important;
	}
	.reveal section .fragment {
		opacity: 1 !important;
		visibility: visible !important;

		-webkit-transform: none !important;
		   -moz-transform: none !important;
		    -ms-transform: none !important;
		        transform: none !important;
	}
	.reveal section img {
		display: block;
		margin: 15px 0px;
		background: rgba(255,255,255,1);
		border: 1px solid #666;
		box-shadow: none;
	}

	.reveal section small {
		font-size: 0.8em;
	}

}  
</script>


<script id="pdf-css" type="application/dynamic-css">
    
/**
 * This stylesheet is used to print reveal.js
 * presentations to PDF.
 *
 * https://github.com/hakimel/reveal.js#pdf-export
 */

* {
	-webkit-print-color-adjust: exact;
}

body {
	margin: 0 auto !important;
	border: 0;
	padding: 0;
	float: none !important;
	overflow: visible;
}

html {
	width: 100%;
	height: 100%;
	overflow: visible;
}

/* Remove any elements not needed in print. */
.nestedarrow,
.reveal .controls,
.reveal .progress,
.reveal .playback,
.reveal.overview,
.fork-reveal,
.share-reveal,
.state-background {
	display: none !important;
}

h1, h2, h3, h4, h5, h6 {
	text-shadow: 0 0 0 #000 !important;
}

.reveal pre code {
	overflow: hidden !important;
	font-family: Courier, 'Courier New', monospace !important;
}

ul, ol, div, p {
	visibility: visible;
	position: static;
	width: auto;
	height: auto;
	display: block;
	overflow: visible;
	margin: auto;
}
.reveal {
	width: auto !important;
	height: auto !important;
	overflow: hidden !important;
}
.reveal .slides {
	position: static;
	width: 100%;
	height: auto;

	left: auto;
	top: auto;
	margin: 0 !important;
	padding: 0 !important;

	overflow: visible;
	display: block;

	-webkit-perspective: none;
	   -moz-perspective: none;
	    -ms-perspective: none;
	        perspective: none;

	-webkit-perspective-origin: 50% 50%; /* there isn't a none/auto value but 50-50 is the default */
	   -moz-perspective-origin: 50% 50%;
	    -ms-perspective-origin: 50% 50%;
	        perspective-origin: 50% 50%;
}

.reveal .slides section {
	page-break-after: always !important;

	visibility: visible !important;
	position: relative !important;
	display: block !important;
	position: relative !important;

	margin: 0 !important;
	padding: 0 !important;
	box-sizing: border-box !important;
	min-height: 1px;

	opacity: 1 !important;

	-webkit-transform-style: flat !important;
	   -moz-transform-style: flat !important;
	    -ms-transform-style: flat !important;
	        transform-style: flat !important;

	-webkit-transform: none !important;
	   -moz-transform: none !important;
	    -ms-transform: none !important;
	        transform: none !important;
}

.reveal section.stack {
	margin: 0 !important;
	padding: 0 !important;
	page-break-after: avoid !important;
	height: auto !important;
	min-height: auto !important;
}

.reveal img {
	box-shadow: none;
}

.reveal .roll {
	overflow: visible;
	line-height: 1em;
}

/* Slide backgrounds are placed inside of their slide when exporting to PDF */
.reveal section .slide-background {
	display: block !important;
	position: absolute;
	top: 0;
	left: 0;
	width: 100%;
	z-index: -1;
}

/* All elements should be above the slide-background */
.reveal section>* {
	position: relative;
	z-index: 1;
}

/* Display slide speaker notes when 'showNotes' is enabled */
.reveal .speaker-notes-pdf {
	display: block;
	width: 100%;
	max-height: none;
	left: auto;
	top: auto;
	z-index: 100;
}

/* Display slide numbers when 'slideNumber' is enabled */
.reveal .slide-number-pdf {
	display: block;
	position: absolute;
	font-size: 14px;
}

</script>


<script>
var style = document.createElement( 'style' );
style.type = 'text/css';
var style_script_id = window.location.search.match( /print-pdf/gi ) ? 'pdf-css' : 'paper-css';
var style_script = document.getElementById(style_script_id).text;
style.innerHTML = style_script;
document.getElementsByTagName('head')[0].appendChild(style);
</script>

</head>
<body>
  <div class="reveal">
    <div class="slides">
<!-- Title Page -->
<section data-background="img/slide-background.svg" class="center" id="title-page">
    <h1 class="title">First Steps with Stan</h1>
  <h1 class="subtitle">Maud goes to Vegas</h1>
    <h2 class="author">Andrew B. Collier</h2>
    <div class="author-info">
        <i class="fa fa-envelope"></i> andrew@exegetic.biz
        |
        <i class="fab fa-twitter"></i> <a href="https://twitter.com/DataWookie">DataWookie</a>
        |
        <i class="fab fa-github"></i> <a href="https://github.com/DataWookie">DataWookie</a>
    </div>
    <h3 class="location">eRum (Budapest)</h3>
    <h3 class="date">15 May 2018</h3>
    <img src="img/logo-toptal.png" class="logo" height="80px">
    <img src="/home/colliera/proj/Z-204-www-exegetic-biz/static/img/exegetic-new-logo-black.svg" class="logo" height="80px">
</section>

<section><section id="section" class="titleslide slide level1" data-background="img/maud-introduction.svg" data-background-size="50%" data-background-position="left bottom"><h1></h1></section><section id="section-1" class="slide level2" data-background="img/slot-machine.jpg">
<h2></h2>
<aside class="notes">
<p>Her weakness is slot machines. She’s positively obsessed by them.</p>
To be honest, Maud is generally rather obsessive. As you’ll see, this will work in our favour.
</aside>
</section><section id="section-2" class="slide level2" data-background="img/maud-love-r.svg" data-background-size="50%" data-background-position="left bottom">
<h2></h2>
<aside class="notes">
<p>Somewhat surprisingly, Maud is also a big fan of R.</p>
It’s been really, um interesting, to have her as a collaborator.
</aside>
</section><section id="data-by-session" class="slide level2">
<h2>Data (by Session)</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sessions <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>details)</code></pre></div>
<pre><code># A tibble: 100 x 7
   session spins  hits wager payout  hit_rate       rtp
     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
 1       1     7     2    10      3 0.2857143 0.3000000
 2       2    19     7    30     29 0.3684211 0.9666667
 3       3    19     3    22      3 0.1578947 0.1363636
 4       4    26     7    30     13 0.2692308 0.4333333
 5       5    23     8    31     35 0.3478261 1.1290323
 6       6    20     8    26     12 0.4000000 0.4615385
 7       7    22     5    30     20 0.2272727 0.6666667
 8       8    22     4    25     10 0.1818182 0.4000000
 9       9    18     4    26      6 0.2222222 0.2307692
10      10    26     8    33     75 0.3076923 2.2727273
# ... with 90 more rows</code></pre>
<aside class="notes">
Maud obsessively records her gambling stats. As a result we have a rather nice, rich data set to play with. At the session level the most important bits of information are the number of spins and the associated number of hits. It might occur to you that the relationship between these two quantities could be modelled as a binomial process.
</aside>
</section><section id="data-by-spin" class="slide level2">
<h2>Data (by Spin)</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sessions <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(session, details) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">unnest</span>()</code></pre></div>
<pre><code># A tibble: 1,972 x 4
   session  spin wager payout
     &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;
 1       1     1     1      1
 2       1     2     1      0
 3       1     3     1      0
 4       1     4     3      0
 5       1     5     2      0
 6       1     6     1      2
 7       1     7     1      0
 8       2     1     2      0
 9       2     2     2      0
10       2     3     1      1
# ... with 1,962 more rows</code></pre>
<aside class="notes">
We can also unpack these data down to the level of individual spins, where we have something that could be modelled as a Bernoulli process.
</aside>
</section><section id="mauds-burning-questions" class="slide level2">
<h2>Maud’s Burning Questions</h2>
<ul>
<li>What is the distribution of the hit rate? (Spoiler: It’s not Normal.)</li>
<li>What is the RTP?</li>
<li>What is the probability of each payline?</li>
</ul>
</section></section>
<section><section id="binomial-process" class="titleslide slide level1"><h1>Binomial Process</h1></section><section id="section-3" class="slide level2 center">
<h2></h2>
<p>Probability distribution for binomial process:</p>
<p><span class="math display">\[
P(k | n, \theta) = \binom{n}{k} \theta^k (1 - \theta)^{n - k}
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(k\)</span> successes in <span class="math inline">\(n\)</span> trials where</li>
<li>the probability of success on any trial is <span class="math inline">\(\theta\)</span>.</li>
</ul>
<aside class="notes">
<p>We’re going to focus on the session level data for the moment. For each session we are considering the number of spins (or “trials”) denoted by <span class="math inline">\(n\)</span> and the number of hits (or “successes”) denoted by <span class="math inline">\(k\)</span>. The probability of success on any spin is <span class="math inline">\(\theta\)</span>.</p>
<p>The binomial distribution allows us to calculate the probability of <span class="math inline">\(k\)</span> given specific values for <span class="math inline">\(n\)</span> and <span class="math inline">\(\theta\)</span>. So this assumes that you know <span class="math inline">\(\theta\)</span>… But what if you don’t? Certainly for most slot machines you don’t know the probability of winning. If you did then you probably wouldn’t be very tempted to play.</p>
Suppose we had observations of <span class="math inline">\(k\)</span> and <span class="math inline">\(n\)</span>, and wanted to estimate <span class="math inline">\(\theta\)</span>.
</aside>
</section><section id="section-4" class="slide level2 center">
<h2></h2>
<p><span class="math display">\[
\text{hit rate} = \theta^* = \frac{\text{hits}}{\text{spins}}
\]</span></p>
<aside class="notes">
Well, the hit rate, <span class="math inline">\(\theta\)</span>, is just the expected number of hits per spin.
</aside>
</section><section id="section-5" class="slide level2 center">
<h2></h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">with</span>(sessions, <span class="kw">sum</span>(hits) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(spins))</code></pre></div>
<pre><code>[1] 0.3128803</code></pre>
<aside class="notes">
<p>So we could lump all of the data together and treat it as a single massive binomial experiment, taking the ratio of the total number of hits to the total number of spins across all sessions.</p>
This would certainly maximise the amount of data and should, in principle, give us a reasonably accurate result. But there is no way to quantify our uncertainty. Such a paucity of information leads to overconfidence in the quality of the estimate.
</aside>
</section><section id="section-6" class="slide level2 center">
<h2></h2>
<p><img src="bayesian-leap_files/figure-revealjs/unnamed-chunk-5-1.png" width="960" /></p>
<pre><code># A tibble: 1 x 2
  session_avg session_std
        &lt;dbl&gt;       &lt;dbl&gt;
1   0.3100018   0.1030164</code></pre>
<p>Confidence interval (95%) extends from 28.9% to 33.1%.</p>
<aside class="notes">
<p>Another approach is to treat each session as an independent binomial experiment, each of which yields an estimate for the hit rate.</p>
<p>Now you can calculate the average and spread. However, there are two problems with this:</p>
<ol type="1">
<li>it ignores the number of spins per session (this could be incorporated by using weighted statistics) and</li>
<li>it assumes that the hit rate is Normally distributed (and it’s not!).</li>
</ol>
Maud is fairly happy with this result, but it does concern her that the confidence interval on the hit rate could, in principle, extend to values less than zero or greater than one. Neither of which is physically possible. There are no constraints on <span class="math inline">\(\theta\)</span>.
</aside>
</section><section class="slide level2">

<p>For session <span class="math inline">\(i\)</span> there were <span class="math inline">\(k_i\)</span> hits from <span class="math inline">\(n_i\)</span> spins.</p>
<p>Assuming that sessions are independent:</p>
<p><span class="math display">\[
P(k | n, \theta) = \prod_i P(k_i | n_i, \theta) = L(\theta; n, k)
\]</span></p>
<aside class="notes">
<p>Let’s consider what the probability distribution looks like for all of the sessions lumped together. If we assume that they are independent, which is a reasonable assumption, then we can simply multiply together the probability distributions for each session. This gives us, one the one hand, a distribution for the number of hits in each of the sessions. But if we think of this as a function of <span class="math inline">\(\theta\)</span> then this gives the likelihood of a particular value of <span class="math inline">\(\theta\)</span> given observed values of <span class="math inline">\(k_i\)</span> and <span class="math inline">\(n_i\)</span>.</p>
<p>If we maximise this function then we have the most likely value of <span class="math inline">\(\theta\)</span> based on all of the data.</p>
<p><strong>Note:</strong></p>
<p><span class="math inline">\(P(k | n, \theta)\)</span> and <span class="math inline">\(L(\theta; n, k)\)</span> are functionally equivalent but</p>
<ul>
<li><span class="math inline">\(P(k | n, \theta)\)</span> is a function of <span class="math inline">\(k\)</span> (and is a probability distribution) and</li>
<li><span class="math inline">\(L(\theta; n, k)\)</span> is a function of <span class="math inline">\(\theta\)</span> (and is <em>not</em> a probability distribution).</li>
</ul>
</aside>
</section><section class="slide level2">

<p>Log-likelihood for binomial process (multiple trials):</p>
<p><span class="math display">\[
LL(\theta; n, k) = \sum_i k_i \log{\theta} + (n_i - k_i) \log{(1 - \theta)}
\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># theta - probability [single value]</span>
<span class="co"># k - number of successes [vector]</span>
<span class="co"># n - number of trials [vector]</span>
<span class="co">#</span>
binom_log_likelihood &lt;-<span class="st"> </span><span class="cf">function</span>(theta, k, n) {
  <span class="kw">sum</span>(k <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(theta) <span class="op">+</span><span class="st"> </span>(n <span class="op">-</span><span class="st"> </span>k) <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta))
}</code></pre></div>
<aside class="notes">
<p>Neglect the binomial coefficient since it’s a constant (as a function of theta).</p>
<p>The log likelihood is generally easier to work with because it gets around the problem of multiplying together many small numbers.</p>
Because the logarithm is a strictly increasing function, the log likelihood is maximised in the same place as the likelihood.
</aside>
</section><section class="slide level2">

<p><img src="bayesian-leap_files/figure-revealjs/unnamed-chunk-12-1.png" width="960" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">parameters <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">arrange</span>(<span class="kw">desc</span>(log_likelihood)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">head</span>()</code></pre></div>
<pre><code># A tibble: 6 x 2
  theta log_likelihood
  &lt;dbl&gt;          &lt;dbl&gt;
1  0.32      -1225.604
2  0.30      -1226.146
3  0.34      -1228.649
4  0.28      -1230.543
5  0.36      -1235.078
6  0.26      -1239.142</code></pre>
<aside class="notes">
<p>In the Maximum Likelihood approach we simply select the value of the parameter which gives the largest likelihood.</p>
<p>The assumption implicit in this approach is that in the absence of data all parameter values are equally probable.</p>
This gives us a point estimate for the parameter. There is no indication of uncertainty. However, the likelihood is a tool which will allow us to get a much better characterisation of the hit rate.
</aside>
</section></section>
<section><section id="deterministic-bayes" class="titleslide slide level1"><h1>Deterministic Bayes</h1></section><section id="bayesian-inference" class="slide level2">
<h2>Bayesian Inference</h2>
<p>Bayes’ Theorem <span class="math display">\[
p(\theta|y, X) = \frac{p(y|X, \theta) p(\theta)}{p(y)} \propto p(y|\theta) p(\theta)
\]</span> where</p>
<ul>
<li><span class="math inline">\(y\)</span> are observations;</li>
<li><span class="math inline">\(X\)</span> are predictors;</li>
<li>prior — <span class="math inline">\(p(\theta)\)</span> is parameter distribution <em>before</em> data;</li>
<li>likelihood — <span class="math inline">\(p(y|X, \theta)\)</span> is probability of data <em>given</em> parameters; and</li>
<li>posterior — <span class="math inline">\(p(\theta|y, X)\)</span> is parameter distribution <em>given</em> data.</li>
</ul>
<aside class="notes">
<p>A Bayesian model is the combination of a prior and a likelihood.</p>
<p>The likelihood characterises the model and gives the probability of the data conditional on the model and choice of parameter.</p>
<p>The prior is specifies what we know about the model parameter before considering the data.</p>
<p>The denominator is sometimes called the “Evidence”. It’s just the marginal distribution of <span class="math inline">\(y\)</span>.</p>
<p>It’s also taken a while for these techniques to take off because they are computationally challenging. Hardware advances have made them eminently feasible.</p>
<p>So much for theory. Analytical expressions are rare in practice.</p>
<p>Confounding features:</p>
<ul>
<li>data are often multi-dimensional;</li>
<li>models have multiple parameters.</li>
</ul>
So evaluating <span class="math inline">\(p(\theta|y, X)\)</span> becomes challenging!
</aside>
</section><section id="section-7" class="slide level2 center">
<h2></h2>
<blockquote>
… the theory of inverse probability is founded upon error and must be wholly rejected. <cite>Sir Ronald Fisher (1925)</cite>
</blockquote>
<aside class="notes">
<p>Historically the Bayesian approach has been marginalised by some eminent statisticians. For example, Ronald Fisher had this to say about inverse probability as Bayesian inference was known in his day. Evidently not everybody is a fan.</p>
In general you can’t apply Bayesian techniques analytically.
</aside>
</section><section class="slide level2">

<p><img src="bayesian-leap_files/figure-revealjs/unnamed-chunk-15-1.png" width="960" /></p>
<aside class="notes">
<p>One approach to applying Bayes’ Theorem is to evaluate the posterior across a regular grid. Not surprisingly this is known as the “Grid Approximation”.</p>
<p>In the grid approximation we consider a finite number of points at which we evaluate the prior and likelihood. Their product then gives us the posterior at each of those points.</p>
<p>There’s a major problem with the grid approximation: it scales poorly with the number of parameters in the model and rapidly becomes computationally intractable.</p>
At each iteration the model effectively learns from the new data. The distribution shifts and becomes narrower. Also note that each iteration uses the results of the previous iteration as a prior.
</aside>
</section><section class="slide level2">

<p><img src="bayesian-leap_files/figure-revealjs/unnamed-chunk-16-1.png" width="960" /></p>
</section><section class="slide level2">

<p><img src="bayesian-leap_files/figure-revealjs/unnamed-chunk-17-1.png" width="960" /></p>
</section></section>
<section><section id="monte-carlo" class="titleslide slide level1"><h1>Monte Carlo</h1></section><section id="vanilla-monte-carlo" class="slide level2">
<h2>Vanilla Monte Carlo</h2>
<p>Generate independent samples of <span class="math inline">\(\theta^{(i)}\)</span>.</p>
<p>To do this we need to have</p>
<p><span class="math display">\[
p(\theta^{(m)} | y)
\]</span></p>
<aside class="notes">
<p>To apply Monte Carlo to a general probability distribution we’d need to apply something like importance sampling. To make this efficient we’d need to know the maximum of the distribution. This would be a significant computational burden.</p>
Fortunately there is an alternative which is very efficient.
</aside>
</section><section id="markov-chain-monte-carlo-mcmc" class="slide level2">
<h2>Markov Chain Monte Carlo (MCMC)</h2>
<p>Generate a series of samples: <span class="math inline">\(\theta^1\)</span>, <span class="math inline">\(\theta^2\)</span>, <span class="math inline">\(\theta^3\)</span>, … <span class="math inline">\(\theta^M\)</span> where <span class="math inline">\(\theta^m\)</span> depends on <span class="math inline">\(\theta^{m-1}\)</span>.</p>
<p>To do this we need to have</p>
<p><span class="math display">\[
p(\theta^{m} | \theta^{m-1}, y)
\]</span></p>
<aside class="notes">
We don’t actually evaluate this probability explicitly. Instead we use some slight of hand.
</aside>
</section><section id="simplified-metropolis-algorithm" class="slide level2">
<h2>Simplified Metropolis Algorithm</h2>
<ol type="1">
<li>Randomly sample <span class="math inline">\(\theta^{(1)}\)</span>.</li>
<li>Set <span class="math inline">\(i = 2\)</span>.</li>
<li>Randomly sample proposal <span class="math inline">\(\theta^{*}\)</span> in the vicinity of <span class="math inline">\(\theta^{i-1}\)</span>.</li>
<li>Sample <span class="math inline">\(u\)</span> uniformly on <span class="math inline">\([0, 1)\)</span>.</li>
<li></li>
</ol>
<p><span class="math display">\[
\theta^{(i)} = \left\{\begin{array}{ll}
\theta^{*} &amp; \text{if } u \cdot p(\theta^{i-1}|y, X) &lt; p(\theta^{*}|y, X) \\
\theta^{(i-1)} &amp; \text{otherwise.}
\end{array}\right.
\]</span></p>
<aside class="notes">
<p>So, the aim of this algorithm is to jump around in parameter space, but in a way that the probability to be at a point is proportional to the function we sample from (this is usually called the target function). In our case this is the posterior defined above.</p>
<p>If the probability of the new point is higher then we always accept. If it’s lower then we accept with a probability that is the ratios of the densities.</p>
<p>The size of the “vicinity” (effectively the step size) determines the acceptance rate of new points. If it’s too large then few new points will be accepted. If it’s too small then the rate of diffusion will be painfully slow.</p>
<p>Without step 5 this would essentially give us diffusion, a random walk. But this step, the Metropolis Acceptance Procedure, ensures that sampling converges to the underlying distribution.</p>
<p>The principle advantage of MCMC is that we don’t need to know the maximum value of the posterior.</p>
<p>Also it only depends on the ratio of posteriors, so these do not need to be normalised.</p>
It might occur to you that the samples are now no longer independent. And you’d be completely correct. We’ll see how this is accounted for.
</aside>
</section><section class="slide level2">

<p><img src="bayesian-leap_files/figure-revealjs/unnamed-chunk-18-1.png" width="960" /></p>
<aside class="notes">
<p>This is an example of a <em>traceplot</em>. You can see where the proposals are being rejected. Ideally we are aiming for good “mixing” across the domain of the distribution.</p>
<p>It might occur to you that the samples are now no longer independent. And you’d be completely correct. We’ll see how this is accounted for.</p>
<p>Metropolis Monte Carlo is better than the Grid Approximation. However in an absolute sense it’s not really all that good. Especially not in higher dimensions.</p>
<p>Metropolis sampling is rather inefficient. Stan actually uses Hamiltonian Monte Carlo. The underlying principles are essentially the same, but this technique samples the parameter space much more efficiently.</p>
<p>Hamiltonian Monte Carlo introduces some coherence into the random walk by using some principles from Physics. Yes, it takes a little longer to generate each sample, but those samples are guaranteed to explore parameter space much more effectively.</p>
<p>The NUTS sampling algorithm is <em>much</em> more efficient than vanilla MCMC.</p>
<p>Treat the parameter vector like the coordinates of a particle on a potential energy surface, where the potential energy is the negative log posterior.</p>
There are more complicated schemes for proposing new locations and the rules for accepting them, but the basic idea is still: (1) pick a new “proposed” location; (2) figure out how much higher or lower that location is compared to your current location; (3) probabilistically stay put or move to that location in a way that respects the overall goal of spending time proportional to height of the location.
</aside>
</section><section id="section-8" class="slide level2 center">
<h2></h2>
<p><img src="/home/colliera/proj/Z-314-datawookie-site/static/img/logo/logo-stan.svg" /></p>
<aside class="notes">
Stan is a high level language for writing statistical models.
</aside>
</section><section id="stan-skeleton" class="slide level2">
<h2>Stan Skeleton</h2>
<script src="https://gist.github.com/DataWookie/61d7a662ecaad87044e1c9f161a5245f.js"></script>
<aside class="notes">
The data block defines variables that have a source outside of Stan.
</aside>
</section><section id="running-stan-from-r" class="slide level2">
<h2>Running Stan from R</h2>
<p>Stan workflow:</p>
<ol type="1">
<li>Data story (informs the model)</li>
<li>Write Stan program (specify likelihood and priors).</li>
<li>Stan parser converts this to C++.</li>
<li>Compile C++.</li>
<li>Execute compiled binary.</li>
<li>Evaluate results. (Possibly return to 2.)</li>
<li>Inference based on posterior sample.</li>
</ol>
<p>Using Stan with R:</p>
<ul>
<li>A <code>.R</code> file. USE FONTAWESOME FILE SYMBOL!!!</li>
<li>A <code>.stan</code> file. USE FONTAWESOME FILE SYMBOL!!!</li>
</ul>
<aside class="notes">
<p>Stan is a standalone system. However it’s a lot easier to run it from within R.</p>
<ol type="1">
<li>Represent a statistical model by writing its log posterior density (up to an normalizing constant that does not depend on the unknown parameters in the model) using the Stan modeling language. We recommend using a separate file with a .stan extension, although it can also be done using a character string within R.</li>
<li>Translate the Stan program to C++ code using the stanc function.</li>
<li>Compile the C++ code to create a DSO (also called a dynamic link library (DLL)) that can be loaded by R.</li>
<li>Run the DSO to sample from the posterior distribution. This is where the magic happens.</li>
<li>Diagnose non-convergence of the MCMC chains.</li>
<li>Conduct inference based on the posterior sample (the MCMC draws from the posterior distribution).</li>
</ol>
<p>As mentioned earlier in the vignette, Stan programs are written in the Stan modeling language, translated to C++ code, and then compiled to a dynamic shared object (DSO). The DSO is then loaded by R and executed to draw the posterior sample. The process of compiling C++ code to DSO sometimes takes a while. When the model is the same, we can reuse the DSO from a previous run. The stan function accepts the optional argument fit, which can be used to pass an existing fitted model object so that the compiled model is reused. When reusing a previous fitted model, we can still specify different values for the other arguments to stan, including passing different data to the data argument.</p>
In addition, if fitted models are saved using functions like save and save.image, RStan is able to save DSOs, so that they can be used across R sessions. To avoid saving the DSO, specify save_dso=FALSE when calling the stan function.
</aside>
</section><section id="section-9" class="slide level2 center">
<h2></h2>
<pre class="text"><code>data {
  int&lt;lower=0&gt; N;
  int hits[N];
  int spins[N];
}
parameters {
  real theta;
}
model {
  hits ~ binomial(spins, theta);       // Likelihood
  theta ~ uniform(0, 1);               // Prior
}</code></pre>
<p>Model with - binomial likelihood and - uniform prior.</p>
<aside class="notes">
<p>The ~ represents a stochastic relationship. An = would indicate a deterministic relationship.</p>
If we did not specify a prior then Stan would use the uniform prior by defaulr.
</aside>
</section><section id="section-10" class="slide level2 center">
<h2></h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rstan)

<span class="kw">options</span>(<span class="dt">mc.cores =</span> parallel<span class="op">::</span><span class="kw">detectCores</span>())

trials &lt;-<span class="st"> </span><span class="kw">list</span>(
  <span class="dt">N       =</span> <span class="kw">nrow</span>(sessions),
  <span class="dt">hits    =</span> sessions<span class="op">$</span>hits,
  <span class="dt">spins   =</span> sessions<span class="op">$</span>spins
)

fit &lt;-<span class="st"> </span><span class="kw">stan</span>(
  <span class="dt">file    =</span> <span class="st">&quot;binomial-no-prior.stan&quot;</span>,
  <span class="dt">data    =</span> trials,
  <span class="dt">chains  =</span> <span class="dv">2</span>,                         <span class="co"># Number of Markov chains</span>
  <span class="dt">warmup  =</span> <span class="dv">500</span>,                       <span class="co"># Number of warmup iterations per chain</span>
  <span class="dt">iter    =</span> <span class="dv">1000</span>                       <span class="co"># Total number of iterations per chain (including warmup)</span>
)</code></pre></div>
<aside class="notes">
<p>This is the corresponding R code. Things to note:</p>
<ul>
<li>it loads the <code>rstan</code> library;</li>
<li>sets the number of cores to be used for parallel execution (otherwise chains are run in series);</li>
<li>data are transferred to Stan via a list;</li>
<li>Stan is invoked via the <code>stan()</code> function; and</li>
<li>parameters specify the number of chains as well as number of warmup and compute iterations per chain.</li>
</ul>
<p>The number of Markov chains to run can be specified using the chains argument to the stan or sampling functions. By default, the chains are executed serially (i.e., one at a time) using the parent R process. There is also an optional cores argument that can be set to the number of chains (if the hardware has sufficient processors and RAM), which is appropriate on most laptops. We typically recommend first calling options(mc.cores=parallel::detectCores()) once per R session so that all available cores can be used without needing to manually specify the cores argument.</p>
Note that there are 1000 iterations in total per chain, of which 500 are warmup, which leaves 500 active iterations per chain. So, in total there are 1000 (2 times 500) active iterations.
</aside>
</section><section class="slide level2">

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">traceplot</span>(fit, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;theta&quot;</span>), <span class="dt">inc_warmup =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p><img src="bayesian-leap_files/figure-revealjs/unnamed-chunk-21-1.png" width="960" /></p>
<aside class="notes">
The chains are uncorrelated, but successive samples within each chain are clearly related to each other.
</aside>
</section><section class="slide level2">

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">class</span>(fit)</code></pre></div>
<pre><code>[1] &quot;stanfit&quot;
attr(,&quot;package&quot;)
[1] &quot;rstan&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit</code></pre></div>
<pre><code>Inference for Stan model: binomial-no-prior.
2 chains, each with iter=1000; warmup=500; thin=1; 
post-warmup draws per chain=500, total post-warmup draws=1000.

          mean se_mean   sd     2.5%      25%      50%      75%    97.5% n_eff Rhat
theta     0.31    0.00 0.01     0.29     0.31     0.31     0.32     0.33   459 1.01
lp__  -1227.36    0.03 0.61 -1229.11 -1227.50 -1227.12 -1226.96 -1226.91   433 1.00

Samples were drawn using NUTS(diag_e) at Thu May 10 07:01:35 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<aside class="notes">
<p><code>n_eff</code> is the effective sample size. This takes into account that the samples are not independent (this is a consequence of correlation in the Markov Chain). This is a vital component of the Monte Carlo version of the Central Limit Theorem.</p>
<p>What’s happening during the warmup phase? The parameters of the MC are getting tuned to optimally explore the parameter space (one aspect of this is choosing the step size).</p>
<p>Maud is immediately more comfortable with this result because, even with a uniform prior, the success rate is constrained to a reasonable domain between 0 and 1.</p>
We don’t actually get the posterior density directly. We get a bunch of samples from it instead.
</aside>
</section><section class="slide level2">

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">samples &lt;-<span class="st"> </span><span class="kw">extract</span>(fit)

<span class="kw">names</span>(samples)</code></pre></div>
<pre><code>[1] &quot;theta&quot; &quot;lp__&quot; </code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">samples<span class="op">$</span>theta <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">head</span>()</code></pre></div>
<pre><code>[1] 0.3125253 0.3089601 0.3194270 0.3055944 0.3276203 0.3157577</code></pre>
</section><section class="slide level2">

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">samples &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(fit)

<span class="kw">head</span>(samples)</code></pre></div>
<pre><code>      theta      lp__
1 0.2988998 -1227.847
2 0.3125688 -1226.911
3 0.3215710 -1227.239
4 0.3190492 -1227.073
5 0.3179268 -1227.018
6 0.3027603 -1227.404</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">nrow</span>(samples)</code></pre></div>
<pre><code>[1] 1000</code></pre>
<aside class="notes">
The number of samples that we get is precisely the total number of active iterations across all chains.
</aside>
</section><section class="slide level2">

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_summary &lt;-<span class="st"> </span><span class="kw">summary</span>(fit, <span class="dt">probs =</span> <span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))
<span class="kw">print</span>(<span class="kw">names</span>(fit_summary))</code></pre></div>
<pre><code>[1] &quot;summary&quot;   &quot;c_summary&quot;</code></pre>
<p>The summary is a list with two components:</p>
<ul>
<li><code>summary</code> - overall summary and</li>
<li><code>c_summary</code> - summary broken down by chain.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(fit_summary<span class="op">$</span>summary)</code></pre></div>
<pre><code>               mean      se_mean          sd          2.5%         97.5%    n_eff      Rhat
theta     0.3130937 0.0004622541 0.009906515     0.2939055     0.3326438 459.2829 1.0079488
lp__  -1227.3597368 0.0293587681 0.610726660 -1229.1053371 -1226.9104936 432.7311 0.9994029</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_summary<span class="op">$</span>summary</code></pre></div>
<pre><code>               mean      se_mean          sd          2.5%         97.5%    n_eff      Rhat
theta     0.3130937 0.0004622541 0.009906515     0.2939055     0.3326438 459.2829 1.0079488
lp__  -1227.3597368 0.0293587681 0.610726660 -1229.1053371 -1226.9104936 432.7311 0.9994029</code></pre>
</section><section id="binomial-model-beta-prior-stan" class="slide level2">
<h2>Binomial Model: Beta Prior (Stan)</h2>
<pre class="text"><code></code></pre>
<aside class="notes">
The beta distribution is the conjugate prior for the binomial likelihood (this means that the corresponding posterior will also be a beta distribution).
</aside>
</section><section class="slide level2">

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Stan is generating draws from the posterior and the results (below) are based on those draws.</span>

<span class="co"># print(fit, pars = c(&quot;mu&quot;, &quot;lp__&quot;), probs = c(0.1, 0.5, 0.9))</span>
<span class="co"># print(fit)</span>

<span class="co"># 80% (inner) and 95% (outer) intervals.</span>
<span class="co">#</span>
<span class="co"># plot(fit)</span>
<span class="co"># </span>
<span class="co"># plot(fit, plotfun = &quot;hist&quot;)</span>
<span class="co"># plot(fit, plotfun = &quot;dens&quot;)</span>

<span class="co"># Autocorrelation between successive samples.</span>
<span class="co">#</span>
<span class="co"># plot(fit, plotfun = &quot;ac&quot;)</span>

<span class="co"># posterior_interval(fit, prob = 0.5) # Is this function from rstanarm?</span>
<span class="co">#</span>
<span class="co"># You can literally say that &quot;there is a 25% chance that it&#39;s smaller than...&quot;</span>

<span class="kw">pairs</span>(fit, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;theta&quot;</span>, <span class="st">&quot;lp__&quot;</span>), <span class="dt">las =</span> <span class="dv">1</span>)</code></pre></div>
<p><img src="bayesian-leap_files/figure-revealjs/unnamed-chunk-29-1.png" width="960" /></p>
<aside class="notes">
Don’t think of a confidence interval. These are quantiles of the density. Close to the median they are stable, but further into the tails they can be noisy.
</aside>
</section><section class="slide level2">

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fit, <span class="dt">show_density =</span> <span class="ot">TRUE</span>, <span class="dt">ci_level =</span> <span class="fl">0.5</span>, <span class="dt">fill_color =</span> <span class="st">&quot;purple&quot;</span>)</code></pre></div>
<pre><code>ci_level: 0.5 (50% intervals)</code></pre>
<pre><code>outer_level: 0.95 (95% intervals)</code></pre>
</section><section id="section-11" class="slide level2">
<h2><img src="bayesian-leap_files/figure-revealjs/unnamed-chunk-30-1.png" width="960" /></h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fit, <span class="dt">plotfun =</span> <span class="st">&quot;hist&quot;</span>, <span class="dt">pars =</span> <span class="st">&quot;theta&quot;</span>, <span class="dt">include =</span> <span class="ot">FALSE</span>)</code></pre></div>
<pre><code>`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
</section><section id="section-12" class="slide level2">
<h2><img src="bayesian-leap_files/figure-revealjs/unnamed-chunk-31-1.png" width="960" /></h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fit, <span class="dt">plotfun =</span> <span class="st">&quot;trace&quot;</span>, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;theta&quot;</span>), <span class="dt">inc_warmup =</span> <span class="ot">TRUE</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;Example of adding title to plot&quot;</span>)</code></pre></div>
<p><img src="bayesian-leap_files/figure-revealjs/unnamed-chunk-32-1.png" width="960" /></p>
</section><section id="section-13" class="slide level2 center">
<h2></h2>
<p><img src="bayesian-leap_files/figure-revealjs/unnamed-chunk-33-1.png" width="960" /></p>
</section><section id="section-14" class="slide level2 center">
<h2></h2>
<pre class="text"><code>data {
  int&lt;lower=0&gt; N;
  real rtp[N];
}
parameters {
  real mu;
  real&lt;lower = 0&gt; sigma;
}
model {
  rtp ~ normal(mu, sigma);
  mu ~ beta(2, 2);                     // Mode = 0.5
}</code></pre>
<aside class="notes">
</aside>
</section><section id="section-15" class="slide level2 center">
<h2></h2>
<pre><code>Inference for Stan model: normal.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

       mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat
mu     0.77    0.00 0.06  0.65  0.73  0.77  0.81  0.88  2719    1
sigma  0.64    0.00 0.04  0.56  0.61  0.64  0.67  0.74  3265    1
lp__  -7.25    0.02 0.94 -9.68 -7.63 -6.96 -6.58 -6.32  1901    1

Samples were drawn using NUTS(diag_e) at Thu May 10 07:20:46 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>THE RTP IS ??? AND THE PLAUSIBLE 95% INTERVAL GOES FROM ??? TO ???</p>
</section><section id="section-16" class="slide level2 center">
<h2></h2>
<p><img src="bayesian-leap_files/figure-revealjs/unnamed-chunk-38-1.png" width="960" /></p>
<aside class="notes">
</aside>
</section><section id="posterior-predictive-check" class="slide level2">
<h2>Posterior Predictive Check</h2>
</section><section id="pay-table" class="slide level2">
<h2>Pay Table</h2>
<p>PUT IN A TABLE WITH paytable$payout</p>
</section><section class="slide level2">

<pre class="text"><code>data {
  int&lt;lower=0&gt; N;
  real rtp[N];
}
parameters {
  real&lt;lower=0, upper=1&gt; mu1;          // Payline 1 -&gt; 1x
  real&lt;lower=0, upper=1&gt; mu2;          // Payline 2 -&gt; 2x
  real&lt;lower=0, upper=1&gt; mu3;          // Payline 3 -&gt; 5x
  real&lt;lower=0, upper=1&gt; mu4;          // Payline 4 -&gt; 10x
  real&lt;lower=0, upper=1&gt; mu5;          // Payline 5 -&gt; 20x
  real&lt;lower=0, upper=1&gt; mu6;          // Payline 6 -&gt; 100x
  real&lt;lower=0&gt; sigma;
}
transformed parameters {
  real&lt;lower=0&gt; pay;
  pay = mu1 * 1 + mu2 * 2 + mu3 * 5 + mu4 * 10 + mu5 * 20 + mu6 * 100;
}
model {
  rtp ~ lognormal(log(pay) - sigma * sigma / 2, sigma);
  //
  // The mean of lognormal() is exp(mu + sigma * sigma / 2).
  //
  mu1 ~ beta(2, 2);                    // Mode = 0.5
  mu2 ~ beta(2, 4);                    // Mode = 0.25
  mu3 ~ beta(2, 5);                    // Mode = 0.2
  mu4 ~ beta(2, 8);                    // Mode = 0.125
  mu5 ~ beta(2, 10);                   // Mode = 0.1
  mu6 ~ beta(2, 16);                   // Mode = 0.0625
}</code></pre>
<aside class="notes">
The priors are based on Maud’s gut feel for the relative frequency of each of the payouts. They carry a little information about the probabilities of each payline, but not a lot.
</aside>
</section><section class="slide level2">

<p><img src="bayesian-leap_files/figure-revealjs/unnamed-chunk-41-1.png" width="960" /></p>
<aside class="notes">
It’s readily apparent here the importance of the warmup iterations: the first few samples for some of the parameters are very far from the core of their distribution. This could be improved by providing more restrictive priors.
</aside>
</section><section class="slide level2">

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(fit)<span class="op">$</span>summary</code></pre></div>
<pre><code>               mean      se_mean          sd          2.5%           25%           50%           75%         97.5%
mu1     0.140183104 1.469255e-03 0.089325826  1.879710e-02  7.177827e-02   0.122664752   0.192136367   0.355484201
mu2     0.068228582 6.855675e-04 0.043359095  1.009871e-02  3.522552e-02   0.059594974   0.092294699   0.172075969
mu3     0.028816792 3.014155e-04 0.019063192  3.628413e-03  1.423612e-02   0.025236838   0.039359326   0.074382807
mu4     0.014604142 1.465113e-04 0.009266188  2.047886e-03  7.471645e-03   0.012931411   0.019858629   0.036488600
mu5     0.007310498 7.466952e-05 0.004625688  1.032144e-03  3.817441e-03   0.006498005   0.009835081   0.018626924
mu6     0.001500567 1.496690e-05 0.000946590  1.915782e-04  7.711689e-04   0.001324914   0.002041584   0.003783854
sigma   0.740078933 1.058005e-03 0.057922464  6.352298e-01  7.001666e-01   0.737313210   0.776424743   0.860629274
pay     0.863032278 1.374708e-03 0.079680070  7.286920e-01  8.064539e-01   0.855920287   0.909859834   1.038541926
lp__  -67.997071371 4.994067e-02 2.062849762 -7.287027e+01 -6.912510e+01 -67.655330546 -66.467193435 -65.058522027
         n_eff      Rhat
mu1   3696.236 0.9991526
mu2   4000.000 0.9995266
mu3   4000.000 0.9995504
mu4   4000.000 1.0004483
mu5   3837.655 1.0004658
mu6   4000.000 1.0003130
sigma 2997.221 1.0006985
pay   3359.532 0.9997769
lp__  1706.186 1.0028480</code></pre>
<p>The overall RTP is NA.</p>
<aside class="notes">
</aside>
</section><section id="predictive-inference" class="slide level2">
<h2>Predictive Inference</h2>
</section><section id="posterior-predictive-distribution" class="slide level2">
<h2>Posterior Predictive Distribution</h2>
<p><span class="math display">\[
p(\tilde{y}|y) = \int_\theta p(\tilde{y}|\theta) p(\theta|y) d\theta
\]</span></p>
</section><section id="what-is-bayes-good-for" class="slide level2">
<h2>What is Bayes Good For?</h2>
<ul>
<li>Models where we care about parameter values.</li>
<li>Models where we want to quantify uncertainty in parameter values.</li>
<li>Generate some derived quantity from the posterior (because we can evaluate any function of the parameters…)</li>
</ul>
<aside class="notes">
So these are not black box Machine Learning models where we simply want to make the best possible predictions. Here we actually care what’s going on inside the box.
</aside>
</section><section id="resources" class="slide level2">
<h2>Resources</h2>
<ul>
<li><a href="http://mc-stan.org/workshops/" class="uri">http://mc-stan.org/workshops/</a></li>
<li><a href="http://mc-stan.org/">Stan web site</a></li>
<li><a href="http://www.mcmchandbook.net/HandbookChapter5.pdf">MCMC Using Hamiltonian Dynamics</a> (Radford Neal)</li>
<li>Blogs:</li>
<li><a href="http://andrewgelman.com/" class="uri">http://andrewgelman.com/</a></li>
</ul>
<!-- ============================================================================================================== -->
<!-- ===                                                                                                        === -->
<!-- === END OF TALK                                                                                            === -->
<!-- ===                                                                                                        === -->
<!-- ============================================================================================================== -->
</section></section>
<section><section id="extra-stuff" class="titleslide slide level1"><h1>Extra Stuff</h1></section><section id="poisson" class="slide level2">
<h2>Poisson</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># trials &lt;- list(</span>
<span class="co">#   N       = nrow(sessions),</span>
<span class="co">#   spins   = sessions$spins</span>
<span class="co"># )</span>
<span class="co"># </span>
<span class="co"># fit &lt;- stan(</span>
<span class="co">#   file    = &quot;poisson.stan&quot;,</span>
<span class="co">#   data    = trials,</span>
<span class="co">#   chains  = 4,</span>
<span class="co">#   warmup  = 1000,</span>
<span class="co">#   iter    = 2000</span>
<span class="co"># )</span></code></pre></div>
</section><section class="slide level2">

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># extract(fit)</span>

<span class="co"># hist(extract(fit)$lambda)</span></code></pre></div>
<aside class="notes">
</aside>
</section><section id="regression" class="slide level2">
<h2>Regression</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># ggplot(sessions, aes(spins, wager)) + geom_jitter()</span></code></pre></div>
</section><section id="regression-stan" class="slide level2">
<h2>Regression Stan</h2>
<pre class="text"><code>data {
  int&lt;lower=0&gt; N;
  vector[N] y;                         // Total wager
  vector[N] x;                         // Number of spins (standardised)
}
parameters {
  real alpha;
  real beta;
  real&lt;lower=0&gt; sigma;
}
model {
  y ~ normal(alpha + beta * x, sigma); // Likelihood
  //
  alpha ~ normal(0, 10);               // Prior (Normal)
  beta ~ normal(0, 10);                // Prior (Normal)
  sigma ~ cauchy(0, 5);                // Prior (Cauchy)
}</code></pre>
<aside class="notes">
Because the number of spins has been standardised, the intercept will be the wager for the average number of spins.
</aside>
</section><section id="regression-stan-r" class="slide level2">
<h2>Regression Stan R</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># summary(fit)</span></code></pre></div>
<aside class="notes">
<p>In a linear model we are saying that the response is normally distributed with a mean that depends on the covariates and a constant standard deviation. Assuming homogeneous noise.</p>
The half-Cauchy is essentially a one-sided, heavy tailed version of the Normal distribution. This constrains the value of sigma to be positive but is not as restrictive as a Normal distribution.
</aside>
</section><section id="shinystan" class="slide level2">
<h2>ShinyStan</h2>
<p>library(shinystan) launch_shinystan(fit) # # Diagnose -&gt; PPcheck “Posterior predictive check” (look at distribution of observed data versus replications - do they look similar? “If our model is a good fit then we should be able to use it to generate data that looks a lot like the data we observed.”)</p>
</section></section>
    </div>
  </div>

  <script src="bayesian-leap_files/reveal.js-3.3.0.1/lib/js/head.min.js"></script>
  <script src="bayesian-leap_files/reveal.js-3.3.0.1/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        slideNumber: true,
        // Push each slide change to the browser history
        history: true,
        // Vertical centering of slides
        center: false,
        // Transition style
        transition: 'none', // none/fade/slide/convex/concave/zoom
        // Transition style for full page slide backgrounds
        backgroundTransition: 'default', // none/fade/slide/convex/concave/zoom



        // Optional reveal.js plugins
        dependencies: [
          { src: 'bayesian-leap_files/reveal.js-3.3.0.1/plugin/notes/notes.js', async: true },
          { src: 'bayesian-leap_files/reveal.js-3.3.0.1/plugin/zoom-js/zoom.js', async: true },
        ]
      });
    </script>
  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>

<script>
  (function() {
    if (window.jQuery) {
      Reveal.addEventListener( 'slidechanged', function(event) {  
        window.jQuery(event.previousSlide).trigger('hidden');
        window.jQuery(event.currentSlide).trigger('shown');
      });
    }
  })();
</script>


  </body>
</html>
